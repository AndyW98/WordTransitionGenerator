We live in an increasingly connected world, with the advent of the internet and the popularity of social media. Only a handful of decades ago, the internet was only used in rare occasions and not many people had even heard of it. Today, every device has access to it, and at an exponentially faster rate. With this boom of interconnectedness, social media has risen to take a large role in many people’s lives. The amount of data that is generated by people using social media is massive, and with that has security concerns, such as who should have access to that data and how should it be used.
	Another technology that has become popular is machine learning, defined as “…the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions,” (“What is Machine Learning?”). Until now, machine learning has been mostly restrained to research purposes and mathematical analysis, however one new usage is in the field of cybersecurity. Having an increasingly efficient and accurate framework specialized to stop malicious attacks is incredibly beneficial, but this comes with the caveat that all new technologies have: what are its ethical ramifications? Machine learning in cybersecurity requires consideration of the data collected and the privacy associated with that data, as well as having confidence that the system can reasonably detect attacks.
	The basis of machine learning is gathering huge sums of data and utilizing that data to specialize in a task. One such task is detecting phishing schemes and responding in an appropriate manner. But what is phishing, and what is it used for? “Phishing is a cybercrime in which a target or targets are contacted by email, telephone or text message by someone posing as a legitimate institution to lure individuals into providing sensitive data […] The information is then used to access important accounts and can result in identity theft and financial loss,” (“What is Phishing?”). By training a system on various emails and data, both with legitimate emails and deliberate phishing attempts, it can then detect malicious emails automatically. However, where this machine is getting those emails from is a cause of concern. There is a breach of privacy if those emails come from personal accounts, and even more so if those people are not notified of such use. On the other hand, that data is being used to protect their accounts from future phishing attacks, as well as other malicious emails.
“Partial adoption of AI applications in cybersecurity will make cyber defence even more porous, and oﬀer a strategic advantage to malicious users who will be able to rely on AI extensively to launch new attacks,” (Mariarosaria Taddeo 4), as stated in her article addressing ethical applications of AI in cybersecurity. She says this in the context that society might reject artificial intelligence, assuming the consequences outweigh the added protection it offers. It is important to remember that computer security is about mitigating risk, not eliminating it. Training a machine on manufactured data can only give so much accuracy before the effectiveness plateaus. Using real-world data from a large database of many users increases that maximum accuracy, and thus protecting those users from more malicious attacks.
Someone having their information stolen today may not affect just that one person, either. For instance, that person may have any number of private information related to their work, their passwords, or even just their social life. If said information about their work was compromised, that company could be open to lawsuits. This would arise in case of client confidentiality, cleared government work, trade secrets, even potentially insider trading resulting from a security breach. If their passwords were compromised, that person likely uses that password on multiple accounts, opening the company up to future attacks. Situations like these make it not just an employee’s issue, but also a company issue. There remains a legal obligation to their clients that they protect their data, and that includes from potential cyber-attacks.
In this sense, a company must do whatever is in their power to improve their machine learning based security software, including using their employee’s emails to make it as accurate as possible. For companies with an email service, this obligation may even bleed into that userbase, since the password example may expose other parts of the company’s services. One could argue that legal obligation outweighs user privacy, especially in matters related to national security. Knowingly omitting data from a training software due to privacy concerns that affects the performance of a security system, thus endangering the confidentiality of government secrets could even be considered a legal offense. 
The alternative argument is that companies should not use private user data in any way without said user’s permission. The most prevalent example that has happened in recent history is the Cambridge Analytica scandal. Carole Cadwalladr from The Guardian writes, “…he came up with a plan to harvest the Facebook profiles of millions of people in the US, and to use their private and personal information to create sophisticated psychological and political profiles. And then target them with political ads designed to work on their particular psychological makeup,” (Carole Cadwalladr), in reference to Cristopher Wylie, the data analyst that helped create Cambridge Analytica. The data was accessed without user permission and retrieved from Facebook. This data was then fed into a machine learning algorithm to make those psychological profiles. The company faced much scrutiny over this breach, and it is widely regarded that what they did was unethical. Using personal data of unwitting users for political purposes is a violation of the trust placed in Facebook, as they have put in their terms of service that collected data would not be used for said political purposes.
The question that remains is where the line is drawn. On one hand, privacy of user data must be conserved and respected. On the other, a more accurate dataset can protect even more people and companies from malicious attacks. Those malicious attackers will not have ethical qualms about using stolen user data for their machine learning based attacks, so there must be a compromise between using accurate datasets and respect of privacy. There have been different methods proposed to anonymize data, and Kellep Charles describes it as modifying the data “…so records cannot be associated with a specific individual, project or company. Unlike the concept of confidentiality, which often means the subjects' identities are known but will be protected by the person evaluating the data, in anonymization, the evaluator does not know the subjects’ identities,” (Kellep Charles). He also suggests methods such as data encryption, substitution, and nulling out as possible ways to keep people’s data separate from themselves. There could also be some sort of opt-in system, where users could volunteer their information, knowing that it will be used for their benefit. Both solutions allow real-world data to be used to train a machine learning algorithm, while remaining ethical.
Another method of anonymization is by changing the way that users generate information. The largest concern of privacy breach is that one’s data is being directly accessed and used without their permission; however, decentralization helps mitigate this concern while remaining relatively accurate. “Federated learning still enables machines to learn from vast amounts of data without centralizing the data or risking revealing and tracing sensitive and private customer information. Thus it promises higher user anonymity since user data does not have to be processed by the machine first,” (Ivona Crnoja). This decentralization of data allows a learning algorithm to look at a set of data as a whole system, which lets people’s data get “lost in the crowd,” so to speak. Their information will still be useful, however, while they remain anonymous as that data was separated from themselves the moment it entered the system.
The largest hurdle in implementing such a system would be a major overhaul in software function. Centralized data stores would have to be reconfigured from the ground up to work with a decentralized data store, which has other impacts such as speed and reliability. The tradeoff would have to be a slower system for more privacy, which many people might be unhappy with. A polling of the user base could be implemented, however either way such a company could go would upset several people, calling into question the morality of forcing the majority’s opinion on the minority. For example, if the majority voted in favor of decentralization, the minority would have to deal with a slower network for a benefit that they didn’t care about or even necessarily want in the first place. The least intrusive movement to this sort of anonymization technique would be offering a new system for people to move towards, however this does not assist in current attempts to improve machine learning.
Besides data collection, there is also the issue of knowing that this new technology will work as intended. As with all new systems, not all the issues can be worked out, especially issues that take months, years, or even decades to show. This can be especially damaging in a critical system such as a security system in an airplane, or in a car. However, that is not to say that new technology should not be adopted. The potential benefits could be that a machine learning based security system can be more efficient and produce many more reliable results than any non-ML security system could give. When considering the adoption of machine learning, one needs to consider the tradeoff of acceptable risks and technological advancements.
Just as data must be collected ethically, it must also be collected reliably and diligently. With machine learning, good data in means you get expected results on the output. This works both ways, though. If one puts garbage data into the machine, you get garbage data out. This is addressed by Martin Giles, who says, “…one risk is that in rushing to get their products to market, companies use training information that hasn’t been thoroughly scrubbed of anomalous data points. That could lead to the algorithm missing some attacks,” (Martin Giles), which is a serious issue. A malicious actor could modify training data and cause serious, potentially difficult to find errors. Using the above example of an airplane, someone looking to attack the components of said airplane with a machine learning based security system could feed it data that acts in a similar way to how their malicious code acts. Then, when that code is acting on the airplane, the security system recognizes that anomalous data as inside of its ‘normal range’ and does nothing, effectively neutralizing its usefulness.
A real-world example of improperly managed data is the Amazon recruiting tool scandal that happened in late 2018. It was an intersection of both bad data and overlooking some key 3 points, resulting in a bias against female applicants. The idea started simple and mundane, to create a hiring algorithm that filters out unqualified candidates and then choose from the resultants the best fit for the job.  The first mistake that was “…because Amazon’s computer models were trained to vet applicants by observing patterns in resumes submitted to the company over a 10-year period,” (Jeffrey Dastin), meaning that the data was unfiltered and completely open to bias. As one would expect, that data did indeed have a bias, because there is a disparity between men and women in the tech field. This led to the teaching algorithm finding less women in the data pool, therefore unintentionally becoming biased against them. Amazon has since rescinded the hiring software, but the lessons learned from faulty data are still there.
The way one collects data is just as important as the training technique, since the latter relies on the former. Collecting random data requires research into what that population has inside of it. This would involve looking into outliers, general trends, obvious biases, et cetera. Amazon learned their lesson with this, when they used a population with a clear bias towards men, leading their machine learning algorithm to see that as a general trend and thus exclude women. The unintended consequences could have been avoided by researching their data pool and cultivating a proper training set. With that said, there are not many ways to be completely sure that a dataset will not result in a surprising output. There could be non-obvious outliers or trends in the data that do not appear in a precursory check, causing unexpected results that have seemingly random causes.
That is not to say that this new technology should not be adopted because of the risks it poses, just that it is only ethical to acknowledge that there are risks, just as there are in any new technology. Counter strategies should be adopted to combat issues such as corrupt or malicious data, as well as any new issues that arise. One such strategy is extensive testing and thorough scrubbing of training data. Another strategy is to have multiple systems that all compare data, such that if any one system is compromised the others can act accordingly. These are not fool-proof solutions, and in security nothing can be, but it remains to be determined if the risks outweigh the benefits.
The next step would be to address the issue with unknowingly biased data. One method is going through and determining at what points there is a bias. In the Amazon example, they ended up removing references to gender in order to counteract the bias in the results. This is called a blindless algorithm, and it is the simpler, more specific way of removing bias. “For example, if you want to predict who should be hired for a position, you might include relevant inputs such as the skills and experience an applicant has and exclude irrelevant information such as gender, race, and age,” (Stas Sajin). However, this method of removing bias has some inherent flaws with its implementation, related more to the general trend of the data than the actual excluded fields themselves. Removing a specific field does not remove references to that field in other portions of data, which is what Amazon encountered when implementing a blindless algorithm. It still had a bias against women, even when they explicitly removed any references to gender, since fields such as “Women’s Chess Club captain,” (Stas Sajin) were still recognized.
Another method is implementing a bias aware algorithm, described as, “Instead of removing variables that are directly tied to biased outcomes, I suggest we do the opposite. In other words, in order to know how much bias exists in your data, you need to allow your model to measure it correctly and then subtract the effect of that bias on the outcome,” (Stas Sajin). This technique allows for removal of expected bias by removing the general trends specifically tied to that one factor. Using such fixed data would prevent the issue that Amazon encountered, instead giving meaningful use to the gathered data. 
Gathering issues with the data would most likely start by using an unfiltered training dataset and running it through a machine learning algorithm. Then, monitor the outputs and point out any inconsistencies. These inconsistencies will be considered biases and will be applied to a bias aware algorithm, and by repeating enough times, ideally, most of the bias will be able to be removed. This way the machine learning algorithm will be able to work as intended and not skew its results towards any biases, such as gender. The applications of a reduced bias dataset are that it allows for a wider range of data to be used and give expected results. Using the above Amazon example again, instead of using just their most recent applications they would be able to gather an even larger trove of tech companies’ resumes and run them through their machine learning algorithm. Then, they would be able to remove the biased trends and have an even more accurate software with less bias than their dataset with manually removed fields. 
However, this method does introduce possible error and reduced accuracy. This stems from the method generalizing the dataset rather than specifically curating or even ignoring certain fields. When data is treated as a whole rather than individual points the edges tend to be cut off, and must be accounted for in the final report, whether that be in a note that acknowledges these errors or a form of correction is applied to the final result. Applying this to a larger picture, utilizing larger data sources can improve accuracy of security software, which means a lower chance of being vulnerable to a malicious attack. On the other hand, removing too much bias may infringe upon the accuracy of the data, which would instead accomplish the exact opposite and lower the accuracy of the security software. Thus, a balance must be achieved, as well as considering acceptable biases to be allowed in the dataset, which would incur extensive testing and further ethical analyses involving acceptable losses and risks.
Machine learning being a new technology applied to cybersecurity is both exciting and concerning. Recent issues regarding data privacy, such as Cambridge Analytica, have cast doubt on the ethical basis of using harvested data. In addition, the reliability and resilience of machine learning has yet to be determined and putting it into critical systems raises concerns about its effectiveness. With regards to data privacy, a balance needs to be struck between the user’s right to privacy and the effectiveness of a security system designed to protect them. This can be accomplished with data anonymization techniques, as well as an opt-in system for only those who are fine with their data being used for their protection. 
Furthermore, the reliability of such security systems needs to stand the test of time and prove that they are more effective and efficient than current systems already in place. This includes improving resilience against bias through appropriate methods. Some of these methods include blindless algorithms, which ignore specific fields in data, and bias aware algorithms, that tune to data to ignore or reduce bias. The risks associated with these should be evaluated and weighed against their potential benefits to overall improve security against malicious attacks.
New technology is not something to fear, rather it is something to research and learn from. The internet brought a new wave of interconnectedness, and it also brought malicious attacks into privacy and security of companies and individuals. Utilizing these new technologies is one of the best ways to remain secure and protected, but the drawbacks must be analyzed and researched. Ethical ramifications are inevitable but still must guide our implementations of new developments such that people’s rights and privacy are not infringed upon, doing more harm than good. There may even be legal requirements that demand some ethics be stretched, especially when sensitive government information is at stake. Learning from the mistakes of previous attempts will help form these ethical guidelines and the acceptable limits of data usage.
